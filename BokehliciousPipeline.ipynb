{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqMlkSL7iKLz",
        "outputId": "14932616-9472-45b6-9a5e-30ec3f039e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 64 kB 717 kB/s \n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 8.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Download the DenseDepth model\n",
        "!wget https://s3-eu-west-1.amazonaws.com/densedepth/nyu.h5 -O ./models/nyu.h5 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2dsiJJjMiRC3"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "import skimage\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
        "from keras.layers import Layer\n",
        "import keras.utils.conv_utils as conv_utils\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "from keras.models import load_model\n",
        "from keras.layers import InputSpec\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "01eBAWpLiS9C"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://github.com/ialhashim/DenseDepth/blob/master/layers.py\n",
        "\n",
        "def data_format_normalized(format):\n",
        "    \"\"\"\n",
        "    Checks if the image data is in the specified format.\n",
        "    Accepted formats are: channels_first or channels_last.\n",
        "    \"\"\"\n",
        "    if format is None:\n",
        "        format = K.image_data_format()\n",
        "    data_format = format.lower()\n",
        "    if data_format not in ('channels_first', 'channels_last'):\n",
        "        raise ValueError(f'Image data format must be either channel-first or channel-last. Recevied {str(format)}')\n",
        "    return data_format\n",
        "\n",
        "\n",
        "class BilinearUpSampling2D(Layer):\n",
        "    def __init__(self, size=(2, 2), data_format=None, **kwargs):\n",
        "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
        "        self.data_format = data_format_normalized(data_format)\n",
        "        self.size = conv_utils.normalize_tuple(size, 2, 'size')\n",
        "        self.input_spec = InputSpec(ndim=4)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\"\n",
        "        Computes the output shape based on the data format specified.\n",
        "        Channels_last: (N, H, W, C)\n",
        "        Channels_first: (N, C, H, W)\n",
        "        \"\"\"\n",
        "        # Calculate (N, H, W, C)\n",
        "        if self.data_format == 'channels_last':\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
        "            return (input_shape[0], height, width, input_shape[3])\n",
        "        # Calculate (N, C, H, W)\n",
        "        elif self.data_format == 'channels_first':\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
        "            return (input_shape[0], input_shape[1], height, width)\n",
        "            \n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        Returns the resized image based on the data format specified.\n",
        "        Uses Bilinear Interpolation for resizing.\n",
        "        \"\"\"\n",
        "        input_shape = K.shape(inputs)\n",
        "        if self.data_format == 'channels_last':\n",
        "            height = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
        "            width = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
        "        elif self.data_format == 'channels_first':\n",
        "            height = self.size[0] * input_shape[2] if input_shape[2] is not None else None\n",
        "            width = self.size[1] * input_shape[3] if input_shape[3] is not None else None\n",
        "\n",
        "        return tf.image.resize(inputs, [height, width], method=tf.image.ResizeMethod.BILINEAR)\n",
        "\n",
        "    def get_config(self):\n",
        "        \"\"\"\n",
        "        Returns the configuration for the model.\n",
        "        \"\"\"\n",
        "        config = {'size': self.size, 'data_format': self.data_format}\n",
        "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "09QRo1Mb0bxa"
      },
      "outputs": [],
      "source": [
        "# Adapted from: https://github.com/ialhashim/DenseDepth/blob/master/utils.py\n",
        "\n",
        "\n",
        "def predict(model, images, minDepth=10, maxDepth=1000, batch_size=2):\n",
        "    # Stack the image thrice if the number of channels is 1\n",
        "    if len(images.shape) < 3: images = np.stack((images, images, images), axis=2)\n",
        "    \n",
        "    # Reshape the image (1, C, H, W)\n",
        "    if len(images.shape) < 4: images = images.reshape((1, images.shape[0], images.shape[1], images.shape[2]))\n",
        "    \n",
        "    # Get the predictions\n",
        "    predictions = model.predict(images, batch_size=batch_size)\n",
        "    return np.clip(maxDepth/predictions, minDepth, maxDepth) / maxDepth\n",
        "\n",
        "\n",
        "def load_images(image_files):\n",
        "    \"\"\"\n",
        "    Functionality to multiple iamges from a directory.\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    for image_file in image_files:\n",
        "        # Open the image and clip it\n",
        "        x = np.clip(np.asarray(Image.open(image_file), dtype=float) / 255, 0, 1)\n",
        "        # Append the images to list\n",
        "        images.append(x)\n",
        "    return np.stack(images, axis=0)\n",
        "\n",
        "\n",
        "def to_multichannel(image):\n",
        "    \"\"\"\n",
        "    Convert 2D image to 3D image by stacking\n",
        "    \"\"\"\n",
        "    # Return the image if in correct shape\n",
        "    if image.shape[2] == 3: return image\n",
        "    # Get the image\n",
        "    image = image[:, :, 0]\n",
        "    # Return the stacked image\n",
        "    return np.stack((image, image, image), axis=2)\n",
        "\n",
        "\n",
        "def display_images(outputs, inputs=None):\n",
        "    \"\"\"\n",
        "    Get the plasma map for the depth map\n",
        "    \"\"\"\n",
        "    # Get the cmap from matplotlib\n",
        "    plasma = plt.get_cmap('plasma')\n",
        "    \n",
        "    shape = (outputs[0].shape[0], outputs[0].shape[1], 3)\n",
        "\n",
        "    images = []\n",
        "\n",
        "    for i in range(outputs.shape[0]):\n",
        "        imgs = []\n",
        "        # Rescale the image (Min max normalization)\n",
        "        rescaled = outputs[i][:, :, 0]\n",
        "        rescaled = rescaled - np.min(rescaled)\n",
        "        rescaled = rescaled / np.max(rescaled)\n",
        "\n",
        "        imgs.append(plasma(rescaled)[:, :, :3])\n",
        "        img_set = np.hstack(imgs)\n",
        "        images.append(img_set)\n",
        "\n",
        "    images = np.stack(images)\n",
        "\n",
        "    return skimage.util.montage(images, multichannel=True, fill=(0, 0, 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vQL3wRYiyTZ-"
      },
      "outputs": [],
      "source": [
        "# Load the models\n",
        "custom_objects = {'BilinearUpSampling2D': BilinearUpSampling2D, 'depth_loss_function': None}\n",
        "depth_model_pre = load_model('/content/nyu.h5', custom_objects=custom_objects, compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dpD4yHsbyLE5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import ImageFilter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MEreqgfSyIXR"
      },
      "outputs": [],
      "source": [
        "class PortUNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PortUNet, self).__init__()\n",
        "\n",
        "    # Define the encoder layers\n",
        "    self.dense_net = models.densenet121(pretrained=True)\n",
        "    self.freeze_pretrained_model(False)\n",
        "\n",
        "    # Define the decoder layers\n",
        "    self.dec1 = nn.Conv2d(1024+1024, 512, 3, padding=1)\n",
        "    self.d_bn1 = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.dec2 = nn.Conv2d(512+512, 256, 5, padding=1)\n",
        "    self.d_bn2 = nn.BatchNorm2d(256)\n",
        "    \n",
        "    self.dec3 = nn.Conv2d(256+256, 128, 5, padding=1)\n",
        "    self.d_bn3 = nn.BatchNorm2d(128)\n",
        "    \n",
        "    self.dec4 = nn.Conv2d(64+128, 16, 7, padding=1)\n",
        "    self.d_bn4 = nn.BatchNorm2d(16)\n",
        "\n",
        "    self.dec5 = nn.Conv2d(16+4, 1, 9, padding=4)\n",
        "\n",
        "  def forward(self, images):\n",
        "    # Encode the input image\n",
        "    features = [images[:, 0:3, :, :]]\n",
        "    for _, layer in self.dense_net.features._modules.items():\n",
        "        features.append(layer(features[-1]))\n",
        "\n",
        "    enc6 = features[11]\n",
        "    enc5 = features[9]\n",
        "    enc4 = features[7]\n",
        "    enc3 = features[5]\n",
        "    enc2 = features[3]\n",
        "    enc1 = features[0]\n",
        "\n",
        "    _, _, H, W = enc5.shape\n",
        "    up1 = nn.functional.interpolate(enc6, size=(H, W), mode='bilinear')\n",
        "    dec1 = F.relu(self.d_bn1(self.dec1(torch.cat((up1, enc5), dim=1))))\n",
        "\n",
        "    _, _, H, W = enc4.shape\n",
        "    up2 = nn.functional.interpolate(dec1, size=(H, W), mode='bilinear')\n",
        "    dec2 = F.relu(self.d_bn2(self.dec2(torch.cat((up2, enc4), dim=1))))\n",
        "\n",
        "    _, _, H, W = enc3.shape\n",
        "    up3 = nn.functional.interpolate(dec2, size=(H, W), mode='bilinear')\n",
        "    dec3 = F.relu(self.d_bn3(self.dec3(torch.cat((up3, enc3), dim=1))))\n",
        "\n",
        "    _, _, H, W = enc2.shape\n",
        "    up4 = nn.functional.interpolate(dec3, size=(H, W), mode='bilinear')\n",
        "    dec4 = F.relu(self.dec4(torch.cat((up4, enc2), dim=1)))\n",
        "\n",
        "    _, _, H, W = enc1.shape\n",
        "    up5 = nn.functional.interpolate(dec4, size=(H, W), mode='bilinear')\n",
        "    dec5 = torch.sigmoid(self.dec5(torch.cat((images, up5), dim=1)))\n",
        "\n",
        "    # Return the final decoded image\n",
        "    # return nn.functional.interpolate(dec5, size=(H, W), mode='bilinear')\n",
        "    return dec5\n",
        "\n",
        "  def freeze_pretrained_model(self, freeze: bool):\n",
        "    for param in self.dense_net.features.parameters():\n",
        "      param.requires_grad = not freeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VPOpauBj0VWv"
      },
      "outputs": [],
      "source": [
        "class DepthUNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DepthUNet, self).__init__()\n",
        "\n",
        "    # Define the encoder layers\n",
        "    self.dense_net = models.densenet121(pretrained=True)\n",
        "    self.freeze_pretrained_model(False)\n",
        "\n",
        "    # Define the decoder layers\n",
        "    self.dec1 = nn.Conv2d(1024+1024, 512, 3, padding=1)\n",
        "    self.d_bn1 = nn.BatchNorm2d(512)\n",
        "\n",
        "    self.dec2 = nn.Conv2d(512+512, 256, 5, padding=1)\n",
        "    self.d_bn2 = nn.BatchNorm2d(256)\n",
        "    \n",
        "    self.dec3 = nn.Conv2d(256+256, 128, 5, padding=1)\n",
        "    self.d_bn3 = nn.BatchNorm2d(128)\n",
        "    \n",
        "    self.dec4 = nn.Conv2d(64+128, 16, 7, padding=1)\n",
        "    self.d_bn4 = nn.BatchNorm2d(16)\n",
        "\n",
        "    self.dec5 = nn.Conv2d(16+3, 1, 9, padding=1)\n",
        "\n",
        "  def forward(self, images):\n",
        "    # Encode the input image\n",
        "    features = [images[:, 0:3, :, :]]\n",
        "    for _, layer in self.dense_net.features._modules.items():\n",
        "        features.append(layer(features[-1]))\n",
        "\n",
        "    enc6 = features[11]\n",
        "    enc5 = features[9]\n",
        "    enc4 = features[7]\n",
        "    enc3 = features[5]\n",
        "    enc2 = features[3]\n",
        "    enc1 = features[0]\n",
        "\n",
        "    # Decode the input image\n",
        "    _, _, H, W = enc5.shape\n",
        "    up1 = nn.functional.interpolate(enc6, size=(H, W), mode='bilinear')\n",
        "    dec1 = F.relu(self.d_bn1(self.dec1(torch.cat((up1, enc5), dim=1))))\n",
        "\n",
        "    _, _, H, W = enc4.shape\n",
        "    up2 = nn.functional.interpolate(dec1, size=(H, W), mode='bilinear')\n",
        "    dec2 = F.relu(self.d_bn2(self.dec2(torch.cat((up2, enc4), dim=1))))\n",
        "\n",
        "    _, _, H, W = enc3.shape\n",
        "    up3 = nn.functional.interpolate(dec2, size=(H, W), mode='bilinear')\n",
        "    dec3 = F.relu(self.d_bn3(self.dec3(torch.cat((up3, enc3), dim=1))))\n",
        "\n",
        "    _, _, H, W = enc2.shape\n",
        "    up4 = nn.functional.interpolate(dec3, size=(H, W), mode='bilinear')\n",
        "    dec4 = F.relu(self.dec4(torch.cat((up4, enc2), dim=1)))\n",
        "\n",
        "    _, _, H, W = enc1.shape\n",
        "    up5 = nn.functional.interpolate(dec4, size=(H, W), mode='bilinear')\n",
        "    dec5 = torch.sigmoid(self.dec5(torch.cat((images[:, 0:3, :, :], up5), dim=1)))\n",
        "\n",
        "    # Return the final decoded image\n",
        "    return nn.functional.interpolate(dec5, size=(H, W), mode='bilinear')\n",
        "\n",
        "  def freeze_pretrained_model(self, freeze: bool):\n",
        "    for param in self.dense_net.features.parameters():\n",
        "      param.requires_grad = not freeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JD-ocO8B7zfl"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467,
          "referenced_widgets": [
            "87ef8f7ac15e46ab9bdc09931ba7d429",
            "41c51a4e0e2e499e9a9f4f5273475bcb",
            "dbb541c8f887443188c4fce735a0b1e5",
            "1412646e2aeb47efb34ad60dd2ad446e",
            "ab6c903dbe5b44d3b4f6441d67617f70",
            "199da60287b34c8885a6f41e2af95fde",
            "c49902d233b24e1aa20b545e9a900c9a",
            "5d9fb4fa50e94185ba7bdc1eed8985e9",
            "e44d60a62e764b45b7f2d552e1aec9c2",
            "750441b119204cf494efa2254721ecea",
            "29a27cc431de4ab1929bf069bfcd0a17"
          ]
        },
        "id": "e9hgqK20wwU1",
        "outputId": "61f1b2b2-9791-4b11-fa36-8ecb3960b804"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87ef8f7ac15e46ab9bdc09931ba7d429",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/30.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6009a8188efa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mportrait_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPortUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mportrait_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_model_fdncpp_lr01_2.pth\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model_fdncpp_lr01_2.pth'"
          ]
        }
      ],
      "source": [
        "portrait_model = PortUNet()\n",
        "portrait_model.load_state_dict(torch.load(\"models/portrait_model.pth\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHRgF4jaz821"
      },
      "outputs": [],
      "source": [
        "depth_model = DepthUNet()\n",
        "depth_model.load_state_dict(torch.load(\"models/depth_model.pth\", map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SnDdK3h0s-Q"
      },
      "outputs": [],
      "source": [
        "portrait_model = portrait_model.to(device)\n",
        "depth_model = depth_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6UeNKS31Fqn"
      },
      "outputs": [],
      "source": [
        "def generate_edge_mask(image):\n",
        "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "    edge = cv2.Sobel(gray, cv2.CV_8U, 1, 1, ksize=5)\n",
        "    return edge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bokeh(image_path: str, blur_intensity: int = 15):\n",
        "    if blur_intensity % 2 == 0:\n",
        "        blur_intensity -= 1\n",
        "\n",
        "    # Default size of the image\n",
        "    size = (320, 416)\n",
        "\n",
        "    # Load the image\n",
        "    input_image = Image.open(image_path)\n",
        "    input_image = input_image.resize(size)\n",
        "\n",
        "    png_file_path = '.examples/img_for_keras.png'\n",
        "    save_image = Image.open(image_path)\n",
        "    save_image = save_image.resize(size)\n",
        "    save_image.save(png_file_path)\n",
        "    input_image_for_depth = load_images(glob.glob(png_file_path))\n",
        "\n",
        "    # Generate edge mask\n",
        "    edges = generate_edge_mask(input_image)\n",
        "\n",
        "    input_image = np.array(input_image)\n",
        "\n",
        "    input_image = TF.to_tensor(input_image)\n",
        "    edges = TF.to_tensor(edges)[0]\n",
        "\n",
        "    # Add edge output\n",
        "    input_image = torch.cat((input_image, edges.unsqueeze(dim=0)), dim=0).unsqueeze(0)\n",
        "\n",
        "    # Get depth map using DenseDepth\n",
        "    depth_mask_dd = predict(depth_model_pre, input_image_for_depth)\n",
        "\n",
        "    # Convert the image to original size by interpolation\n",
        "    _, H, W, _ = depth_mask_dd.shape\n",
        "    depth_mask_dd = F.interpolate(torch.Tensor(depth_mask_dd).permute(0, 3, 1, 2), (H * 2, W * 2), mode='bilinear')\n",
        "    depth_mask_dd = depth_mask_dd.permute(0, 2, 3, 1)\n",
        "\n",
        "    # portrait_model.eval()\n",
        "    portrait_mask = portrait_model(input_image)\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # Perform edge processing\n",
        "        portrait_mask = portrait_mask[0]\n",
        "        portrait_mask = (portrait_mask - portrait_mask.mean()) / portrait_mask.std()\n",
        "        portrait_mask = np.where(portrait_mask.cpu().numpy() > 0, 1, 0)\n",
        "        EDGE_WIDTH = 9\n",
        "        output_pil = Image.fromarray(np.uint8(portrait_mask.squeeze(0))).convert('RGB')\n",
        "        border_mask = output_pil.filter(ImageFilter.MaxFilter(size=EDGE_WIDTH))\n",
        "        \n",
        "        border_mask = np.array(border_mask).transpose(2, 0, 1) - portrait_mask.squeeze()\n",
        "        fine_mask = np.logical_or(np.logical_and(border_mask[0], edges), portrait_mask.squeeze(0))\n",
        "        fine_mask = cv2.GaussianBlur(fine_mask.cpu().numpy(), (EDGE_WIDTH, EDGE_WIDTH), cv2.BORDER_DEFAULT)\n",
        "\n",
        "        # Get depth of the image\n",
        "        depth_mask_our = depth_model(input_image)\n",
        "\n",
        "        # Apply depthwise gaussian blur to the images\n",
        "        gaussian_stacks = []\n",
        "        for sigma in range(1, 20, 4):\n",
        "            gaussian_stacks.append(cv2.GaussianBlur(input_image[0, :3, :, :].cpu().numpy().transpose(1, 2, 0), (blur_intensity * sigma, blur_intensity * sigma), 5))\n",
        "\n",
        "        depth_mask_dd = np.repeat(depth_mask_dd, 3).reshape(gaussian_stacks[0].shape)\n",
        "        depth_mask_dd = (depth_mask_dd - depth_mask_dd.min()) / (depth_mask_dd.max() - depth_mask_dd.min())\n",
        "        \n",
        "        count = 0\n",
        "        recreated_image = input_image[0, :3, :, :].numpy().transpose(1, 2, 0)\n",
        "        for depth in [0, 0.2, 0.4, 0.6, 0.8]:\n",
        "            recreated_image = np.where(np.logical_and(depth <= depth_mask_dd, depth_mask_dd <= depth + 0.2), \n",
        "                                        gaussian_stacks[count],\n",
        "                                        recreated_image)\n",
        "            count += 1\n",
        "\n",
        "        \n",
        "        fine_mask = fine_mask.repeat(3).reshape(recreated_image.shape)\n",
        "        bokehlicious_image = np.where(fine_mask, input_image[0][:3, :, :].permute(1, 2, 0), recreated_image)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        plt.figure(figsize=(25, 25))\n",
        "        plt.subplot(1, 5, 1)\n",
        "        plt.imshow(input_image[0, :3, :, :].permute(1, 2, 0))\n",
        "        plt.title('Original Image')\n",
        "\n",
        "        plt.subplot(1, 5, 2)\n",
        "        plt.title('Dense depth map')\n",
        "        plt.imshow(np.where(depth_mask_dd, depth_mask_dd, 0)[:, :, 0], cmap=plt.get_cmap('plasma'))\n",
        "\n",
        "        plt.subplot(1, 5, 3)\n",
        "        plt.imshow(fine_mask[:, :, 0])\n",
        "        plt.title('Portrait Mask')\n",
        "\n",
        "        # plt.figure(figsize=(20, 20))\n",
        "        plt.subplot(1, 5, 4)\n",
        "        plt.imshow(recreated_image)\n",
        "        plt.title('Depthwise blur')\n",
        "\n",
        "        plt.subplot(1, 5, 5)\n",
        "        plt.title('Bokehlicious image')\n",
        "        plt.imshow(bokehlicious_image)\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzm25uxj9B-g"
      },
      "outputs": [],
      "source": [
        "blur_intensity=3\n",
        "jpg_image_path = '/content/test.jpg'\n",
        "if blur_intensity % 2 == 0:\n",
        "    blur_intensity -= 1\n",
        "\n",
        "# Default size of the image\n",
        "size = (320, 416)\n",
        "\n",
        "# Load the image\n",
        "input_image = Image.open(jpg_image_path)\n",
        "input_image = input_image.resize(size)\n",
        "\n",
        "png_file_path = 'examples/sample.png'\n",
        "save_image = Image.open(jpg_image_path)\n",
        "save_image = save_image.resize(size)\n",
        "save_image.save(png_file_path)\n",
        "input_image_for_depth = load_images(glob.glob(png_file_path))\n",
        "\n",
        "# Generate edge mask\n",
        "edges = generate_edge_mask(input_image)\n",
        "\n",
        "input_image = np.array(input_image)\n",
        "\n",
        "input_image = TF.to_tensor(input_image)\n",
        "edges = TF.to_tensor(edges)[0]\n",
        "\n",
        "# Add edge output\n",
        "input_image = torch.cat((input_image, edges.unsqueeze(dim=0)), dim=0).unsqueeze(0)\n",
        "\n",
        "# Get depth map using DenseDepth\n",
        "depth_mask_dd = predict(depth_model_pre, input_image_for_depth)\n",
        "\n",
        "# Convert the image to original size by interpolation\n",
        "_, H, W, _ = depth_mask_dd.shape\n",
        "depth_mask_dd = F.interpolate(torch.Tensor(depth_mask_dd).permute(0, 3, 1, 2), (H * 2, W * 2), mode='bilinear')\n",
        "depth_mask_dd = depth_mask_dd.permute(0, 2, 3, 1)\n",
        "\n",
        "# portrait_model.eval()\n",
        "portrait_mask = portrait_model(input_image)\n",
        "with torch.no_grad():\n",
        "    \n",
        "    # Perform edge processing\n",
        "    portrait_mask = portrait_mask[0]\n",
        "    portrait_mask = (portrait_mask - portrait_mask.mean()) / portrait_mask.std()\n",
        "    portrait_mask = np.where(portrait_mask.cpu().numpy() > 0, 1, 0)\n",
        "    EDGE_WIDTH = 9\n",
        "    output_pil = Image.fromarray(np.uint8(portrait_mask.squeeze(0))).convert('RGB')\n",
        "    border_mask = output_pil.filter(ImageFilter.MaxFilter(size=EDGE_WIDTH))\n",
        "    \n",
        "    border_mask = np.array(border_mask).transpose(2, 0, 1) - portrait_mask.squeeze()\n",
        "    fine_mask = np.logical_or(np.logical_and(border_mask[0], edges), portrait_mask.squeeze(0))\n",
        "    fine_mask = cv2.GaussianBlur(fine_mask.cpu().numpy(), (EDGE_WIDTH, EDGE_WIDTH), cv2.BORDER_DEFAULT)\n",
        "\n",
        "    # Get depth of the image\n",
        "    depth_mask_our = depth_model(input_image)\n",
        "\n",
        "    # Apply depthwise gaussian blur to the images\n",
        "    gaussian_stacks = []\n",
        "    for sigma in range(1, 20, 4):\n",
        "        gaussian_stacks.append(cv2.GaussianBlur(input_image[0, :3, :, :].cpu().numpy().transpose(1, 2, 0), (blur_intensity * sigma, blur_intensity * sigma), 5))\n",
        "\n",
        "    depth_mask_dd = np.repeat(depth_mask_dd, 3).reshape(gaussian_stacks[0].shape)\n",
        "    depth_mask_dd = (depth_mask_dd - depth_mask_dd.min()) / (depth_mask_dd.max() - depth_mask_dd.min())\n",
        "    \n",
        "    count = 0\n",
        "    recreated_image = input_image[0, :3, :, :].numpy().transpose(1, 2, 0)\n",
        "    for depth in [0, 0.2, 0.4, 0.6, 0.8]:\n",
        "        recreated_image = np.where(np.logical_and(depth <= depth_mask_dd, depth_mask_dd <= depth + 0.2), \n",
        "                                    gaussian_stacks[count],\n",
        "                                    recreated_image)\n",
        "        count += 1\n",
        "\n",
        "    \n",
        "    fine_mask = fine_mask.repeat(3).reshape(recreated_image.shape)\n",
        "    bokehlicious_image = np.where(fine_mask, input_image[0][:3, :, :].permute(1, 2, 0), recreated_image)\n",
        "\n",
        "with torch.no_grad():\n",
        "    plt.figure(figsize=(25, 25))\n",
        "    plt.subplot(1, 5, 1)\n",
        "    plt.imshow(input_image[0, :3, :, :].permute(1, 2, 0))\n",
        "    plt.title('Original Image')\n",
        "\n",
        "    plt.subplot(1, 5, 2)\n",
        "    plt.title('Dense depth map')\n",
        "    plt.imshow(np.where(depth_mask_dd, depth_mask_dd, 0)[:, :, 0], cmap=plt.get_cmap('plasma'))\n",
        "\n",
        "    plt.subplot(1, 5, 3)\n",
        "    plt.imshow(fine_mask[:, :, 0])\n",
        "    plt.title('Portrait Mask')\n",
        "\n",
        "    # plt.figure(figsize=(20, 20))\n",
        "    plt.subplot(1, 5, 4)\n",
        "    plt.imshow(recreated_image)\n",
        "    plt.title('Depthwise blur')\n",
        "\n",
        "    plt.subplot(1, 5, 5)\n",
        "    plt.title('Bokehlicious image')\n",
        "    plt.imshow(bokehlicious_image)\n",
        "\n",
        "    plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1412646e2aeb47efb34ad60dd2ad446e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_750441b119204cf494efa2254721ecea",
            "placeholder": "​",
            "style": "IPY_MODEL_29a27cc431de4ab1929bf069bfcd0a17",
            "value": " 30.8M/30.8M [00:00&lt;00:00, 56.2MB/s]"
          }
        },
        "199da60287b34c8885a6f41e2af95fde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29a27cc431de4ab1929bf069bfcd0a17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41c51a4e0e2e499e9a9f4f5273475bcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_199da60287b34c8885a6f41e2af95fde",
            "placeholder": "​",
            "style": "IPY_MODEL_c49902d233b24e1aa20b545e9a900c9a",
            "value": "100%"
          }
        },
        "5d9fb4fa50e94185ba7bdc1eed8985e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "750441b119204cf494efa2254721ecea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87ef8f7ac15e46ab9bdc09931ba7d429": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41c51a4e0e2e499e9a9f4f5273475bcb",
              "IPY_MODEL_dbb541c8f887443188c4fce735a0b1e5",
              "IPY_MODEL_1412646e2aeb47efb34ad60dd2ad446e"
            ],
            "layout": "IPY_MODEL_ab6c903dbe5b44d3b4f6441d67617f70"
          }
        },
        "ab6c903dbe5b44d3b4f6441d67617f70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c49902d233b24e1aa20b545e9a900c9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbb541c8f887443188c4fce735a0b1e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d9fb4fa50e94185ba7bdc1eed8985e9",
            "max": 32342954,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e44d60a62e764b45b7f2d552e1aec9c2",
            "value": 32342954
          }
        },
        "e44d60a62e764b45b7f2d552e1aec9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
